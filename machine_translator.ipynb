{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-19T14:39:07.906110Z",
     "iopub.status.busy": "2023-06-19T14:39:07.904490Z",
     "iopub.status.idle": "2023-06-19T14:39:08.985717Z",
     "shell.execute_reply": "2023-06-19T14:39:08.984324Z",
     "shell.execute_reply.started": "2023-06-19T14:39:07.906036Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls /notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-19T14:38:05.974511Z",
     "iopub.status.busy": "2023-06-19T14:38:05.974161Z",
     "iopub.status.idle": "2023-06-19T14:38:09.055864Z",
     "shell.execute_reply": "2023-06-19T14:38:09.054101Z",
     "shell.execute_reply.started": "2023-06-19T14:38:05.974486Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip list > /notebooks/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-06-19T14:37:20.445956Z",
     "iopub.status.busy": "2023-06-19T14:37:20.445535Z",
     "iopub.status.idle": "2023-06-19T14:37:23.888603Z",
     "shell.execute_reply": "2023-06-19T14:37:23.887532Z",
     "shell.execute_reply.started": "2023-06-19T14:37:20.445924Z"
    },
    "id": "Nc1jqZZHzEsQ",
    "outputId": "152ec7b8-c3dc-4839-f3d0-945770f264f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece datasets tensorflow_text einops subword-nmt langid tensorrt --no-binary :all: tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQmabPo1qwV0"
   },
   "source": [
    "# Neural Machine Translation for English and Russian: A BERT-Based Approach\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwbGqMCHruOE"
   },
   "source": [
    "## Mathematical knowledge for Text-Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHJGSX2JkSvs"
   },
   "source": [
    "### initial step: imports and necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:37.429039Z",
     "iopub.status.busy": "2023-06-15T19:55:37.428459Z",
     "iopub.status.idle": "2023-06-15T19:55:37.440741Z",
     "shell.execute_reply": "2023-06-15T19:55:37.439338Z",
     "shell.execute_reply.started": "2023-06-15T19:55:37.429039Z"
    },
    "id": "8F3zAEyqPh59",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "import random\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import string\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import transformers as trns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:40.157066Z",
     "iopub.status.busy": "2023-06-15T19:55:40.156700Z",
     "iopub.status.idle": "2023-06-15T19:55:40.166432Z",
     "shell.execute_reply": "2023-06-15T19:55:40.165047Z",
     "shell.execute_reply.started": "2023-06-15T19:55:40.157039Z"
    },
    "id": "MUTynOZ3QGir",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return[item for item in dir(tokenizers.en) if not item.startswith('_')]\n",
    "\n",
    "    parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "    for name, new_dim in parsed.items():\n",
    "      old_dim = self.shapes.get(name, None)\n",
    "\n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETERS AND CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:44.155183Z",
     "iopub.status.busy": "2023-06-15T19:55:44.154653Z",
     "iopub.status.idle": "2023-06-15T19:55:44.163573Z",
     "shell.execute_reply": "2023-06-15T19:55:44.161993Z",
     "shell.execute_reply.started": "2023-06-15T19:55:44.155138Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS_LENGTH = 128\n",
    "BUFFER_SIZE=None\n",
    "BATCH_SIZE=64\n",
    "\n",
    "MAX_VOCAB_SIZE = 150000 # MAXIMUM AMOUNT OF WORDS\n",
    "UNITS = 256\n",
    "\n",
    "NUM_HEADS = 8\n",
    "DENSE_LAYER_NEURONS = 2048\n",
    "NUM_LAYER = 6\n",
    "DROPOUT_RATE=0.1\n",
    "\n",
    "en_vocab_size=None\n",
    "ru_vocab_size=None\n",
    "\n",
    "max_subword_length = 10\n",
    "\n",
    "reserved_tokens = [\"[START]\", \"[END]\", \"[UNK]\", \"[SEP]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZF6rFH4JVyA"
   },
   "source": [
    "### Data Collection:\n",
    "Collect a large parallel corpus of text data for the language pairs that you want to translate. A parallel corpus contains sentences in one language and their corresponding translations in another language. You can use existing parallel corpora or create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:46.698069Z",
     "iopub.status.busy": "2023-06-15T19:55:46.696712Z",
     "iopub.status.idle": "2023-06-15T19:55:46.703769Z",
     "shell.execute_reply": "2023-06-15T19:55:46.700999Z",
     "shell.execute_reply.started": "2023-06-15T19:55:46.698024Z"
    },
    "id": "N_YCQRoTQuRW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path_to_file = pathlib.Path(\"/content/drive/MyDrive/rus.txt\")\n",
    "# # corpus = pathlib.Path(\"/content/drive/MyDrive/corpus\")\n",
    "\n",
    "# rus_path = \"/content/drive/MyDrive/corpus.en_ru.1m.ru\"\n",
    "# en_path = \"/content/drive/MyDrive/corpus.en_ru.1m.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:47.926097Z",
     "iopub.status.busy": "2023-06-15T19:55:47.925541Z",
     "iopub.status.idle": "2023-06-15T19:55:47.934396Z",
     "shell.execute_reply": "2023-06-15T19:55:47.932661Z",
     "shell.execute_reply.started": "2023-06-15T19:55:47.926058Z"
    }
   },
   "outputs": [],
   "source": [
    "rus_path = \"../datasets/yandex_rus/corpus.en_ru.1m.ru\"\n",
    "en_path = \"../datasets/yandex_en/corpus.en_ru.1m.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:50.741730Z",
     "iopub.status.busy": "2023-06-15T19:55:50.741271Z",
     "iopub.status.idle": "2023-06-15T19:55:50.750072Z",
     "shell.execute_reply": "2023-06-15T19:55:50.746670Z",
     "shell.execute_reply.started": "2023-06-15T19:55:50.741700Z"
    },
    "id": "Jbzt-LloYyaw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "langid.set_languages(['en', 'ru'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kYeO3ooJcY9",
    "tags": []
   },
   "source": [
    "### Data Preprocessing:\n",
    "Preprocess the data by cleaning, tokenizing, and normalizing the text. Therafter, forming a vocabulary from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:52.768369Z",
     "iopub.status.busy": "2023-06-15T19:55:52.767852Z",
     "iopub.status.idle": "2023-06-15T19:55:52.776698Z",
     "shell.execute_reply": "2023-06-15T19:55:52.775118Z",
     "shell.execute_reply.started": "2023-06-15T19:55:52.768332Z"
    },
    "id": "l4_IJYjj9Wc7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lower_and_split_punct(text):\n",
    "    # Нормализация символов.\n",
    "    # text = tf_text.normalize_utf8(text, \"NFKD\")\n",
    "    text = tf.strings.lower(text, encoding=\"utf-8\")\n",
    "    # Оставляем пробелы, буквы а-я a-z и выбранные знаки препинания.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-zа-я.?!,¿ёйъь-]', '')\n",
    "    # Добавляем пробелы вокруг знаков препинания.\n",
    "    text = tf.strings.regex_replace(text, r'([.?!,¿-])', r'  \\1')\n",
    "    # Удаляем лишние пробелы.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:56.513433Z",
     "iopub.status.busy": "2023-06-15T19:55:56.512897Z",
     "iopub.status.idle": "2023-06-15T19:55:56.520272Z",
     "shell.execute_reply": "2023-06-15T19:55:56.518524Z",
     "shell.execute_reply.started": "2023-06-15T19:55:56.513402Z"
    },
    "id": "zYiEeiHJu6uV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = tf.constant(\"Последний я ё ъь\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2023-06-15T19:55:57.327747Z",
     "iopub.status.busy": "2023-06-15T19:55:57.326198Z",
     "iopub.status.idle": "2023-06-15T19:55:57.337601Z",
     "shell.execute_reply": "2023-06-15T19:55:57.336762Z",
     "shell.execute_reply.started": "2023-06-15T19:55:57.327726Z"
    },
    "id": "ZsaGK1chvArw",
    "outputId": "1ec601a2-8b6f-47dc-9ff8-7e0007391a11",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lower_and_split_punct(txt).numpy().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip7wryjifqbQ",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Dataset with parallel sentences sized 467000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxTpZXSRQ9V_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data=pd.read_csv(path,delimiter='\\t',header=None, encoding='utf8')\n",
    "    data=data.iloc[:,:2]\n",
    "    data.rename(columns={0: \"English\", 1: \"Russian\"}, inplace=True)\n",
    "    print(data.head())\n",
    "    context = data['English'].values\n",
    "    target = data['Russian'].values\n",
    "    return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xs8r2zX5f7YJ",
    "outputId": "9d9fa58e-190d-4c25-8ca1-595d480bba72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_raw, context_raw = load_data(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdR2dhMbgFDj",
    "outputId": "51d64b62-2ce7-41ad-e9c3-e0d9cb80c89c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(context_raw))\n",
    "print(len(target_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "jCifWG4pgJZC",
    "outputId": "8e573a19-1168-4aca-a59a-11a6557e7741"
   },
   "outputs": [],
   "source": [
    "context_raw[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "4LYRt9LwgLDz",
    "outputId": "9bcfcea6-1b65-43c1-f104-d3d9e3cbe333"
   },
   "outputs": [],
   "source": [
    "target_raw[898]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mJEj9RTgOu3"
   },
   "outputs": [],
   "source": [
    "# a = np.vectorize(lower_and_split_punct)\n",
    "# target_raw = a(target_raw)\n",
    "# context_raw = a(context_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRzFjSxjgSd1",
    "outputId": "62b89af9-553a-4675-e7ed-e89cb1fd125a"
   },
   "outputs": [],
   "source": [
    "len(context_raw)//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkMJPe1EgVGx"
   },
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(context_raw)\n",
    "# # BUFFER_SIZE = 20000\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "# is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "# train_raw = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
    "#     .shuffle(BUFFER_SIZE)\n",
    "#     .batch(BATCH_SIZE))\n",
    "# val_raw = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
    "#     .shuffle(BUFFER_SIZE)\n",
    "#     .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJ1sMd1yYCur"
   },
   "outputs": [],
   "source": [
    "# for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "#   # print(example_context_strings[:5])\n",
    "#   # print(example_context_strings[1])\n",
    "#   # print(example_target_strings[1])\n",
    "#   # print(example_target_strings[:5])\n",
    "#   print(len(example_context_strings))\n",
    "#   print(len(example_target_strings))\n",
    "#   # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0d2BmuWf0Vw",
    "tags": []
   },
   "source": [
    "##### Dataset with parallel sentences sized 1000000 from Yandex Parallel Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:04.812049Z",
     "iopub.status.busy": "2023-06-15T19:56:04.811474Z",
     "iopub.status.idle": "2023-06-15T19:56:04.822452Z",
     "shell.execute_reply": "2023-06-15T19:56:04.821524Z",
     "shell.execute_reply.started": "2023-06-15T19:56:04.812005Z"
    },
    "id": "pD700plqVwle",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_yandex_corpus(rus_path, en_path):\n",
    "    with open(rus_path, 'rb') as russian_file:\n",
    "      russian_sentences = russian_file.readlines()\n",
    "\n",
    "    with open(en_path, 'r', encoding='utf-8') as english_file:\n",
    "      english_sentences = english_file.readlines()\n",
    "\n",
    "    data = {\n",
    "        'Russian': russian_sentences,\n",
    "        'English': english_sentences\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    target = df['Russian'].values\n",
    "    context = df['English'].values\n",
    "\n",
    "    filter_context = []\n",
    "    filter_target = []\n",
    "\n",
    "    for src, trg in zip(context, target):\n",
    "      if len(src)>=4 and len(trg)>=4:\n",
    "        filter_context.append(src)\n",
    "        filter_target.append(trg)\n",
    "\n",
    "    target = np.array(filter_target)\n",
    "    context = np.array(filter_context)\n",
    "\n",
    "    # filtered_source_sentences = []\n",
    "    # filtered_target_sentences = []\n",
    "\n",
    "    # for source_sentence, target_sentence in zip(context, target):\n",
    "    # # Classify the source sentence language\n",
    "    #     source_lang, _ = langid.classify(source_sentence)\n",
    "\n",
    "    # # Classify the target sentence language\n",
    "    #     target_lang, _ = langid.classify(target_sentence)\n",
    "\n",
    "    # # Only keep the sentences where both the source and target are identified as the expected languages\n",
    "    #     if source_lang == 'en' and target_lang == 'ru':\n",
    "    #         filtered_source_sentences.append(source_sentence)\n",
    "    #         filtered_target_sentences.append(target_sentence)\n",
    "\n",
    "    # return filtered_source_sentences, filtered_target_sentences\n",
    "    return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:06.237291Z",
     "iopub.status.busy": "2023-06-15T19:56:06.237048Z",
     "iopub.status.idle": "2023-06-15T19:56:11.924949Z",
     "shell.execute_reply": "2023-06-15T19:56:11.923683Z",
     "shell.execute_reply.started": "2023-06-15T19:56:06.237291Z"
    },
    "id": "GTC6rAM1dAc_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_raw_corpus, context_raw_corpus = load_data_yandex_corpus(rus_path, en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:11.930567Z",
     "iopub.status.busy": "2023-06-15T19:56:11.930025Z",
     "iopub.status.idle": "2023-06-15T19:56:11.939444Z",
     "shell.execute_reply": "2023-06-15T19:56:11.937885Z",
     "shell.execute_reply.started": "2023-06-15T19:56:11.930526Z"
    },
    "id": "XcHRf1kDdrBb",
    "outputId": "36d39065-e5a4-4ad0-c2fb-c962135631f6"
   },
   "outputs": [],
   "source": [
    "print(len(target_raw_corpus))\n",
    "print(len(context_raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:11.941712Z",
     "iopub.status.busy": "2023-06-15T19:56:11.941289Z",
     "iopub.status.idle": "2023-06-15T19:56:11.952396Z",
     "shell.execute_reply": "2023-06-15T19:56:11.950676Z",
     "shell.execute_reply.started": "2023-06-15T19:56:11.941676Z"
    },
    "id": "Ih8ZVWJKeDt-",
    "outputId": "91114f5c-e739-4804-f6a8-663f01ca91ed"
   },
   "outputs": [],
   "source": [
    "target_raw_corpus[500].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:11.955109Z",
     "iopub.status.busy": "2023-06-15T19:56:11.954676Z",
     "iopub.status.idle": "2023-06-15T19:56:11.966998Z",
     "shell.execute_reply": "2023-06-15T19:56:11.965777Z",
     "shell.execute_reply.started": "2023-06-15T19:56:11.955069Z"
    },
    "id": "gJb9xtBKeGdn",
    "outputId": "364d5a4f-e3bd-493a-f1ff-23908771a2df"
   },
   "outputs": [],
   "source": [
    "context_raw_corpus[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7cF_O1Ea4lV",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:12.212565Z",
     "iopub.status.busy": "2023-06-15T19:56:12.212081Z",
     "iopub.status.idle": "2023-06-15T19:56:12.235485Z",
     "shell.execute_reply": "2023-06-15T19:56:12.234235Z",
     "shell.execute_reply.started": "2023-06-15T19:56:12.212522Z"
    },
    "id": "OtcCDK6qa7h3"
   },
   "outputs": [],
   "source": [
    "target_raw_corpus, context_raw_corpus = np.concatenate((target_raw_corpus, target_raw)), np.concatenate((context_raw_corpus, context_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGiEnfkAa_8Q"
   },
   "outputs": [],
   "source": [
    "# Generate a permutation index\n",
    "permutation = np.random.permutation(len(target_raw_corpus))\n",
    "\n",
    "# Shuffle both arrays using the permutation index\n",
    "target_raw_corpus = target_raw_corpus[permutation]\n",
    "context_raw_corpus = context_raw_corpus[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Subword different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "rus_path = \"/notebooks/trg.txt.subwords\"\n",
    "en_path = \"/notebooks/ctx.txt.subwords\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "print()\n",
    "# Build the subword vocabulary\n",
    "tokenizer.build_vocab(rus_path, en_path)\n",
    "\n",
    "# Save the vocabulary\n",
    "vocab_path = \"bert_subword_vocab.txt\"\n",
    "tokenizer.save_vocabulary(vocab_path)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "encoded_tokens = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_tokens)\n",
    "\n",
    "print(decoded_text)\n",
    "print(encoded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw_corpus)\n",
    "max_vocab_size = 100000 \n",
    "BATCH_SIZE = 1\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw_corpus),)) < 0.8\n",
    "\n",
    "context_raw_train = context_raw_corpus[is_train]\n",
    "context_raw_val = context_raw_corpus[~is_train]\n",
    "\n",
    "target_raw_train = target_raw_corpus[is_train]\n",
    "target_raw_val = target_raw_corpus[~is_train]\n",
    "\n",
    "# train_raw = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((context_raw_train, target_raw_train))\n",
    "#     .shuffle(BUFFER_SIZE)\n",
    "#     .batch(BATCH_SIZE))\n",
    "# val_raw = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((context_raw_val, target_raw_val))\n",
    "#     .shuffle(BUFFER_SIZE)\n",
    "#     .batch(BATCH_SIZE))\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_train, target_raw_train)))\n",
    "    # .shuffle(BUFFER_SIZE))\n",
    "    # .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_val, target_raw_val)))\n",
    "    # .shuffle(BUFFER_SIZE))\n",
    "    # .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, ru in train_raw.take(1):\n",
    "    print(en.numpy().decode(\"utf-8\"))\n",
    "    print()\n",
    "    print(ru.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_ds = train_raw.map(lambda context, target: (lower_and_split_punct(context), lower_and_split_punct(target)), tf.data.AUTOTUNE)\n",
    "val_raw_ds = val_raw.map(lambda context, target: (lower_and_split_punct(context), lower_and_split_punct(target)), tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 100000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_path = \"../datasets/yandex_rus/corpus.en_ru.1m.ru\"\n",
    "en_path = \"../datasets/yandex_en/corpus.en_ru.1m.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Read the context and target files\n",
    "ctx_file = \"ctx.txt.subwords\"\n",
    "trg_file = \"trg.txt.subwords\"\n",
    "\n",
    "with open(ctx_file, \"r\") as f:\n",
    "    ctx_text = f.read()\n",
    "\n",
    "with open(trg_file, \"r\") as f:\n",
    "    trg_text = f.read()\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = text.BertTokenizer()\n",
    "\n",
    "# Tokenize the text\n",
    "ctx_tokens = tokenizer.tokenize(ctx_text)\n",
    "trg_tokens = tokenizer.tokenize(trg_text)\n",
    "\n",
    "# Convert tokens to integer sequences\n",
    "ctx_token_ids = tokenizer.convert_tokens_to_ids(ctx_tokens)\n",
    "trg_token_ids = tokenizer.convert_tokens_to_ids(trg_tokens)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "ctx_dataset = tf.data.Dataset.from_tensor_slices(ctx_token_ids)\n",
    "trg_dataset = tf.data.Dataset.from_tensor_slices(trg_token_ids)\n",
    "\n",
    "# Print some example token sequences\n",
    "for ctx_tokens, trg_tokens in zip(ctx_dataset.take(5), trg_dataset.take(5)):\n",
    "    print(\"Context tokens:\", ctx_tokens)\n",
    "    print(\"Target tokens:\", trg_tokens)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_tokenizer = tf_text.BertTokenizer(rus_path, **bert_tokenizer_params)\n",
    "en_tokenizer = tf_text.BertTokenizer(en_path, **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for en, ru in train_raw_ds.take(1):\n",
    "#     print(en.numpy().decode(\"utf-8\"))\n",
    "#     print()\n",
    "#     print(ru.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_en = tf_text.BertTokenizer(en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = trns.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"ты очень добрый\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, ru in train_raw_ds.take(1):\n",
    "    en_tokens = tokenizer.tokenize(en.numpy().decode(\"utf-8\"))\n",
    "    print(\"English text: \", en.numpy().decode(\"utf-8\"))\n",
    "    print(\"Tokens English: \", en_tokens)\n",
    "    print(\"English ids: \", tokenizer.convert_tokens_to_ids(en_tokens))\n",
    "    print()\n",
    "    rus_tokens = tokenizer.tokenize(ru.numpy().decode(\"utf-8\"))\n",
    "    print(\"Russian text: \", ru.numpy().decode(\"utf-8\"))\n",
    "    print(\"Tokens Russian: \", rus_tokens)\n",
    "    print(\"Russian ids: \", tokenizer.convert_tokens_to_ids(rus_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [item for item in dir(tokenizer) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in dir(tokenizer) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s32MGXsoEzgr",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cp8XBF_Ed0MG"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw_corpus)\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw_corpus),)) < 0.8\n",
    "\n",
    "context_raw_train = context_raw_corpus[is_train]\n",
    "context_raw_val = context_raw_corpus[~is_train]\n",
    "\n",
    "target_raw_train = target_raw_corpus[is_train]\n",
    "target_raw_val = target_raw_corpus[~is_train]\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_train, target_raw_train)))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_val, target_raw_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVwobGJcegIA"
   },
   "outputs": [],
   "source": [
    "train_raw_ds = train_raw.map(lambda context, target: (lower_and_split_punct(context), lower_and_split_punct(target)), tf.data.AUTOTUNE)\n",
    "val_raw_ds = val_raw.map(lambda context, target: (lower_and_split_punct(context), lower_and_split_punct(target)), tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, en in train_raw_ds.take(1):\n",
    "    print(context_subword_processor.encode(cnt.numpy()))\n",
    "    print(len(context_subword_processor.encode(en.numpy().decode(\"utf-8\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqOHLCO1e5kE"
   },
   "outputs": [],
   "source": [
    "num_elements = 1\n",
    "\n",
    "# Take the first num_elements from the train_ds dataset\n",
    "sample_elements = train_raw_ds.take(num_elements)\n",
    "\n",
    "# Iterate over the sample_elements dataset and print the context and target\n",
    "for context, target in sample_elements:\n",
    "    print(\"Context:\", context)\n",
    "    print(\"Target:\", target.numpy().decode(\"utf-8\"))\n",
    "    # print(\"Context:\", len(context))\n",
    "    # print(\"Target:\", len(target))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jA3cVKd3gBzm"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "context_subword_processor = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus( # convertation of a text to encoded tokens using subword\n",
    "    (context.numpy() for context, target in train_raw_ds),\n",
    "    target_vocab_size=MAX_VOCAB_SIZE,\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    max_subword_length=max_subword_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_subword_processor.save_to_file(\"/notebooks/ctx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaNimFIugE7p"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "target_subword_processor = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (target.numpy() for context, target in train_raw_ds),\n",
    "    target_vocab_size=MAX_VOCAB_SIZE,\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    max_subword_length=max_subword_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_subword_processor.save_to_file(\"/notebooks/trg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yOb1LXDmBxc"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "context_encoded_train = [context_subword_processor.encode(s.numpy()) for s, t in train_raw]\n",
    "context_encoded_val = [context_subword_processor.encode(s.numpy()) for s, t in val_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XRu2iBtmC7Q"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "target_encoded_train = [target_subword_processor.encode(t.numpy()) for s, t in train_raw]\n",
    "target_encoded_val = [target_subword_processor.encode(t.numpy()) for s, t in val_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZE5I7P-yb2xl"
   },
   "outputs": [],
   "source": [
    "start_text = \"[START]\"\n",
    "end_text = \"[END]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Enp9ENeWb-uz"
   },
   "outputs": [],
   "source": [
    "context_subword_processor.subwords.insert(0, start_text)\n",
    "context_subword_processor.subwords.insert(1, end_text)\n",
    "target_subword_processor.subwords.insert(0, start_text)\n",
    "target_subword_processor.subwords.insert(1, end_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"START\"\n",
    "end_token = \"END\"\n",
    "\n",
    "for subword in context_subword_processor.subwords:\n",
    "    if subword == start_token:\n",
    "        print(f\"Found '{start_token}' token.\")\n",
    "    elif subword == end_token:\n",
    "        print(f\"Found '{end_token}' token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in dir(context_subword_processor) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in dir(target_subword_processor) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h26xQiPqmUtW"
   },
   "outputs": [],
   "source": [
    "START_TOKEN, END_TOKEN = [0], [1] # adding additional tokens to start and end a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgD9ME99cN_s"
   },
   "outputs": [],
   "source": [
    "print(context_subword_processor.vocab_size)\n",
    "print(target_subword_processor.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1U1KFmm04lc8"
   },
   "outputs": [],
   "source": [
    "context_subword_processor.subwords[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_subword_processor.subwords[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4KFF_3hmZqw"
   },
   "outputs": [],
   "source": [
    "# context_vocab_size = context_subword_processor.vocab_size + 2\n",
    "# target_vocab_size = target_subword_processor.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(context, target):\n",
    "    \n",
    "    context_tensor = context_subword_processor.encode(context.numpy())\n",
    "    context_tensor = tf.constant(context_tensor[:MAX_TOKENS_LENGTH])\n",
    "    context_tensor = tf.\n",
    "    \n",
    "    target_tensor = target_subword_processor.encode(target_numpy())\n",
    "    target_tensor_in = \n",
    "    \n",
    "    # context_tensor = tf.keras.utils.pad_sequences(context, maxlen=MAX_TOKENS_LENGTH, dtype=\"int64\", padding='post', truncating='post')\n",
    "    context_tensor = tf.keras.preprocessing.sequence.pad_sequences(context, maxlen=MAX_TOKENS_LENGTH, dtype=\"int64\", padding='post', truncating='post')\n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target, maxlen=MAX_TOKENS_LENGTH, dtype=\"int64\", padding='post', truncating='post')\n",
    "    # target_tensor = tf.keras.utils.pad_sequences(target, maxlen=MAX_TOKENS_LENGTH, dtype=\"int64\", padding='post', truncating='post')\n",
    "    target_tensor_in = target_tensor[:, :-1]\n",
    "    target_tensor_out = target_tensor[:, 1:]\n",
    "    return (context_tensor, target_tensor_in), target_tensor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.ragged.constant(target_encoded_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(context: list, target: list):\n",
    "    context_tensor = tf.ragged.constant(context)\n",
    "    target_tensor = tf.ragged.constant(target)\n",
    "\n",
    "    # Pad sequences to a maximum length\n",
    "    context_tensor = context_tensor.to_tensor(default_value=0)\n",
    "    target_tensor = target_tensor.to_tensor(default_value=0)\n",
    "\n",
    "    target_tensor_in = target_tensor[:, :-1]\n",
    "    target_tensor_out = target_tensor[:, 1:]\n",
    "\n",
    "    return (context_tensor, target_tensor_in), target_tensor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_encoded_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(context_encoded_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = prepare_batch(context_encoded_train[:15], target_encoded_train[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0][0].shape)\n",
    "print(a[0][1].shape)\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEF6ed45p_im"
   },
   "outputs": [],
   "source": [
    "def seq_dataset(context: list, target: list):\n",
    "    return (tf.data.Dataset.from_tensor_slices((context, target))\n",
    "            .shuffle(BUFFER_SIZE)\n",
    "            .map(prepare_dataset, tf.data.AUTOTUNE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_dataset(ds):\n",
    "    return (ds\n",
    "            .map(prepare_dataset, tf.data.AUTOTUNE)\n",
    "            .shuffle(BUFFER_SIZE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuyEINxQtDfP"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_ds = seq_dataset(context_encoded_train, target_encoded_train)\n",
    "val_ds = seq_dataset(context_encoded_val, target_encoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_ds = seq_dataset(train_raw_ds)\n",
    "val_ds = seq_dataset(val_raw_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLw0cCijuB3N"
   },
   "outputs": [],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "  print(ex_context_tok[0, :].numpy())\n",
    "  print()\n",
    "  print(ex_tar_in[0, :].numpy())\n",
    "  print(ex_tar_out[0, :].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jfidRvt3nsZ"
   },
   "outputs": [],
   "source": [
    "print(context_subword_processor.decode(ex_context_tok[0, :]))\n",
    "print(target_subword_processor.decode(ex_tar_in[0, :]))\n",
    "print(target_subword_processor.decode(ex_tar_out[0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-zl5MdAbcnu",
    "tags": []
   },
   "source": [
    "#### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:35.328107Z",
     "iopub.status.busy": "2023-06-15T19:56:35.328107Z",
     "iopub.status.idle": "2023-06-15T19:56:41.455422Z",
     "shell.execute_reply": "2023-06-15T19:56:41.454307Z",
     "shell.execute_reply.started": "2023-06-15T19:56:35.328107Z"
    },
    "id": "cHQQdk6wdk-i",
    "outputId": "4bd7c7f3-aa5e-4b41-8266-a8a2fdc5c0b7"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw_corpus)\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw_corpus),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_corpus[is_train], target_raw_corpus[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_corpus[~is_train], target_raw_corpus[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:41.458563Z",
     "iopub.status.busy": "2023-06-15T19:56:41.457444Z",
     "iopub.status.idle": "2023-06-15T19:56:44.796547Z",
     "shell.execute_reply": "2023-06-15T19:56:44.794867Z",
     "shell.execute_reply.started": "2023-06-15T19:56:41.458556Z"
    },
    "id": "hdUQS7EIdmnt"
   },
   "outputs": [],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "  # print(example_context_strings[:5])\n",
    "  # print(example_context_strings[1])\n",
    "  # print(example_target_strings[1])\n",
    "  # print(example_target_strings[:5])\n",
    "  print(len(example_context_strings))\n",
    "  print(len(example_target_strings))\n",
    "  # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhrtYa3JE_no"
   },
   "source": [
    "##### Tokenizing context words, in our case english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:44.799241Z",
     "iopub.status.busy": "2023-06-15T19:56:44.798444Z",
     "iopub.status.idle": "2023-06-15T19:56:44.818291Z",
     "shell.execute_reply": "2023-06-15T19:56:44.816860Z",
     "shell.execute_reply.started": "2023-06-15T19:56:44.799241Z"
    },
    "id": "EKIG8bjKbF-T"
   },
   "outputs": [],
   "source": [
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=lower_and_split_punct,\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:44.821839Z",
     "iopub.status.busy": "2023-06-15T19:56:44.820887Z",
     "iopub.status.idle": "2023-06-15T19:56:51.295630Z",
     "shell.execute_reply": "2023-06-15T19:56:51.291719Z",
     "shell.execute_reply.started": "2023-06-15T19:56:44.821783Z"
    },
    "id": "IpovRNaTbMmx"
   },
   "outputs": [],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context, tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cdeMLpEErHC"
   },
   "source": [
    "##### Tokenizing target words, in our case russian words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-15T19:56:51.297581Z",
     "iopub.status.idle": "2023-06-15T19:56:51.298310Z",
     "shell.execute_reply": "2023-06-15T19:56:51.297920Z",
     "shell.execute_reply.started": "2023-06-15T19:56:51.297920Z"
    },
    "id": "kDBN7S-PZPl5"
   },
   "outputs": [],
   "source": [
    "len(context_text_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-15T19:56:51.300218Z",
     "iopub.status.idle": "2023-06-15T19:56:51.300717Z",
     "shell.execute_reply": "2023-06-15T19:56:51.300717Z",
     "shell.execute_reply.started": "2023-06-15T19:56:51.300717Z"
    },
    "id": "jP5_M8oPFP_9"
   },
   "outputs": [],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=lower_and_split_punct,\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target, tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:53.292663Z",
     "iopub.status.busy": "2023-06-15T19:56:53.292262Z",
     "iopub.status.idle": "2023-06-15T19:56:53.299490Z",
     "shell.execute_reply": "2023-06-15T19:56:53.297387Z",
     "shell.execute_reply.started": "2023-06-15T19:56:53.292634Z"
    },
    "id": "VeIZALpL3BGS"
   },
   "outputs": [],
   "source": [
    "# target_text_processor.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:54.475618Z",
     "iopub.status.busy": "2023-06-15T19:56:54.474554Z",
     "iopub.status.idle": "2023-06-15T19:56:54.937664Z",
     "shell.execute_reply": "2023-06-15T19:56:54.936613Z",
     "shell.execute_reply.started": "2023-06-15T19:56:54.475584Z"
    },
    "id": "Fx5gXkKi3NKL"
   },
   "outputs": [],
   "source": [
    "len(target_text_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPICdNG0GYcs"
   },
   "source": [
    "We divided our parallel sentences into 64 batches. We vectorized words and assigned at each word its own index id, and this is a vocabulary.\n",
    "And we gave one batch sized 64 to vectorized and here what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T19:56:57.847582Z",
     "iopub.status.busy": "2023-06-15T19:56:57.846295Z",
     "iopub.status.idle": "2023-06-15T19:56:58.010133Z",
     "shell.execute_reply": "2023-06-15T19:56:58.008811Z",
     "shell.execute_reply.started": "2023-06-15T19:56:57.847533Z"
    },
    "id": "04SNR9kkFRqJ"
   },
   "outputs": [],
   "source": [
    "example_tokens_c = context_text_processor(example_context_strings)\n",
    "example_tokens_c[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PE3tSlix65cE"
   },
   "outputs": [],
   "source": [
    "example_tokens_c.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AN2shEJIXqE"
   },
   "outputs": [],
   "source": [
    "example_target_strings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg4mxB1EINYJ"
   },
   "outputs": [],
   "source": [
    "example_tokens_t = target_text_processor(example_target_strings)\n",
    "example_tokens_t[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOKFkQBKIk0K"
   },
   "outputs": [],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens_c = context_vocab[example_tokens_c[0].numpy()]\n",
    "' '.join(tokens_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tW0bNSIaIrsK"
   },
   "outputs": [],
   "source": [
    "target_vocab = np.array(target_text_processor.get_vocabulary())\n",
    "tokens_t = target_vocab[example_tokens_t[0].numpy()]\n",
    "' '.join(tokens_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZW0xXI-NhlN",
    "tags": []
   },
   "source": [
    "## Processing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Datasets` of strings are transformed into 0-padded tensors of token IDs via the `process_text` function listed below. For training with `keras.Model.fit,` it also changes from a `((context, target))` pair to a `((context, target_in), target_out))` pair. `(inputs, labels)` pairs are what Keras anticipates; the inputs are `((context, target_in))` and the labels are `target_out`. Target_in and Target_out differ from each other in that they are moved apart by one step, making the label the subsequent token at each position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Processing with subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "  context = context_subword_processor.encode(context).to_tensor()\n",
    "  target = target_subword_processor.encode(target)\n",
    "  targ_in = target[:,:-1].to_tensor()\n",
    "  targ_out = target[:,1:].to_tensor()\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_size = context_subword_processor.vocab_size\n",
    "ru_vocab_size = target_subword_processor.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Processing with word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "282EjuFCr7lS",
    "tags": []
   },
   "source": [
    "##### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "9BhXGREaOjF5",
    "outputId": "96e09c74-eb10-4e32-d2b3-e1bcc6585ff5"
   },
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "  context = context_text_processor(context).to_tensor()\n",
    "  target = target_text_processor(target)\n",
    "  targ_in = target[:,:-1].to_tensor()\n",
    "  targ_out = target[:,1:].to_tensor()\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-WiXM4ZQrXY"
   },
   "source": [
    "targ_in for all words except last one\n",
    "\n",
    "targ_out for all except first one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYjVjRe6P-6K",
    "outputId": "5a1b3e4b-5dfd-4ecc-bc1a-8f1c655e9fd7"
   },
   "outputs": [],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "  print(ex_context_tok[0, :].numpy())\n",
    "  print()\n",
    "  print(ex_tar_in[0, :].numpy())\n",
    "  print(ex_tar_out[0, :].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_size = len(context_text_processor.get_vocabulary())\n",
    "ru_vocab_size = len(target_text_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF58z_MwsD1S",
    "tags": []
   },
   "source": [
    "## Structure and What are transformers and attention used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06LQhcKXgyVb"
   },
   "source": [
    "##### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6gI5Uqae4Ss"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis] # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths) # (1, depth)\n",
    "  angle_rads = positions * angle_rates  # seq, depth\n",
    "\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1\n",
    "  )\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVKVh1f7e-Xp"
   },
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(length=DENSE_LAYER_NEURONS, depth=UNITS)\n",
    "\n",
    "# Check the shape.\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "# Plot the dimensions.\n",
    "# plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "# plt.ylabel('Depth')\n",
    "# plt.xlabel('Position')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "# pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC_tJ81EfCgv"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "    self.pos_encoding = positional_encoding(length=DENSE_LAYER_NEURONS, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    x*= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NiaJ8zSgKds"
   },
   "outputs": [],
   "source": [
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQwxExl6ga32"
   },
   "outputs": [],
   "source": [
    "for (en, ru_in), ru_out in train_ds.take(1):\n",
    "  # print(example_context_strings[1])\n",
    "  # print(example_target_strings[1])\n",
    "  break\n",
    "print(en.shape)\n",
    "print(ru_in.shape)\n",
    "print(ru_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fm_0WSf5gQP5"
   },
   "outputs": [],
   "source": [
    "embed_en = PositionalEmbedding(vocab_size=en_vocab_size, d_model=UNITS)\n",
    "embed_ru = PositionalEmbedding(vocab_size=ru_vocab_size, d_model=UNITS)\n",
    "\n",
    "en_emb = embed_en(en)\n",
    "ru_emb = embed_ru(ru_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZd7EKUTgUq9",
    "outputId": "1b6e9b10-a24a-424f-87cf-96893f503014"
   },
   "outputs": [],
   "source": [
    "print(en_emb._keras_mask[0])\n",
    "print(ru_emb._keras_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHD4QTjhT0VB"
   },
   "source": [
    "#### The Encoder/Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEbAPLwCh1E4"
   },
   "source": [
    "The goal of the encoder is to process the context sequence into a sequence of vectors that are useful for the decoder as it attempts to predict the next output for each timestep. Since the context sequence is constant, there is no restriction on how information can flow in the encoder, so use a bidirectional-RNN to do the processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eATXBK5l-21d"
   },
   "source": [
    "Takes a list of token IDs (from context_text_processor).\n",
    "Looks up an embedding vector for each token (Using a layers.Embedding).\n",
    "Processes the embeddings into a new sequence (Using a bidirectional layers.GRU).\n",
    "Returns the processed sequence. This will be passed to the attention head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5dAK6KElSVS"
   },
   "source": [
    "The Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV39hXgxlUtG"
   },
   "source": [
    "The attention layer lets the decoder access the information extracted by the encoder. It computes a vector from the entire context sequence, and adds that to the decoder's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21EM7m0Bhsm_"
   },
   "source": [
    "##### FeedForwarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAriO-gzhvQd"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=DROPOUT_RATE):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model),\n",
    "        tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    # self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "    self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.batch_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqgcaOaurIlp"
   },
   "source": [
    "##### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMgAu02JrIlq"
   },
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWBL7ovZrIlq"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.batch_norm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebo7fpK7rIlr"
   },
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x\n",
    "    )\n",
    "    x = self.add([x, attn_output])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3tYMe6irIlr"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask=True\n",
    "    )\n",
    "    x = self.add([x, attn_output])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f3oAMHMm2PT"
   },
   "source": [
    "The Decoder\n",
    "\n",
    "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
    "\n",
    "1. It looks up embeddings for each token in the target sequence.\n",
    "2. It uses an RNN to process the target sequence, and keep track of what it has generated so far.\n",
    "3. It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output.\n",
    "4. At each location in the output it predicts the next token.\n",
    "\n",
    "When training, the model predicts the next word at each location. So it's important that the information only flows in one direction through the model. The decoder uses a unidirectional (not bidirectional) RNN to process the target sequence.\n",
    "\n",
    "When running inference with this model it produces one word at a time, and those are fed back into the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UA7rUxgpg4WU"
   },
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ipk_VPqogmSE"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=DROPOUT_RATE):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMGZbS6_hAGQ",
    "outputId": "e0dd5dde-aed7-4ee6-b45f-48e30637c904"
   },
   "outputs": [],
   "source": [
    "sample = EncoderLayer(d_model=UNITS, num_heads=NUM_HEADS, dff=DENSE_LAYER_NEURONS)\n",
    "print(ru_emb.shape)\n",
    "print(sample(ru_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P68OEzQik9XM"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=DROPOUT_RATE):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-M8Sb1WVlA3i",
    "outputId": "f0c1e498-9131-417e-e101-9d41af5bddbc"
   },
   "outputs": [],
   "source": [
    "# Instantiate the encoder.\n",
    "sample_encoder = Encoder(num_layers=NUM_LAYER,\n",
    "                         d_model=UNITS,\n",
    "                         num_heads=NUM_HEADS,\n",
    "                         dff=DENSE_LAYER_NEURONS,\n",
    "                         vocab_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "sample_encoder_output = sample_encoder(en, training=False)\n",
    "\n",
    "# Print the shape.\n",
    "print(en.shape)\n",
    "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ijh4_u2xl59-"
   },
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCRsAIyAl-W1"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZI3i5hVoCF9"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=DROPOUT_RATE):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TELIRY6noE1n",
    "outputId": "a4d3527e-12ba-4a52-8387-f5750de2175c"
   },
   "outputs": [],
   "source": [
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=NUM_LAYER,\n",
    "                         d_model=UNITS,\n",
    "                         num_heads=NUM_HEADS,\n",
    "                         dff=DENSE_LAYER_NEURONS,\n",
    "                         vocab_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=ru_in,\n",
    "    context=en_emb)\n",
    "\n",
    "# Print the shapes.\n",
    "print(ru_in.shape)\n",
    "print(en_emb.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wAMI1PZpGeJ",
    "outputId": "11129f0d-13d0-41ea-c0fb-5b1d9a966d2b"
   },
   "outputs": [],
   "source": [
    "sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6628v9sppUNh"
   },
   "source": [
    "##### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRh95-X9pYO6"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=DROPOUT_RATE):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMdMb2YRpahw"
   },
   "source": [
    "##### Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkdwVp79qUid"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=NUM_LAYER,\n",
    "    d_model=UNITS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DENSE_LAYER_NEURONS,\n",
    "    input_vocab_size=en_vocab_size,\n",
    "    target_vocab_size=ru_vocab_size,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YztPtgvoq3EL",
    "outputId": "eea3a846-68c4-40c3-a68b-05d6e4b03c97"
   },
   "outputs": [],
   "source": [
    "output = transformer((en, ru_in))\n",
    "\n",
    "print(ru_in.shape)\n",
    "print(en.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uyy1pstRrEg6",
    "outputId": "cf7bf3cf-e845-4a05-a47f-cd4af19470fa"
   },
   "outputs": [],
   "source": [
    "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
    "print(attn_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kq3QkIN7rUJc",
    "outputId": "e5d08e3d-6bfc-4959-fea4-b99eb2b0962d"
   },
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oqgZoy_rYbQ"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw1KQVRNravo"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62HepUkDrhV5"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(UNITS)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "ZBYn5v88r0mY",
    "outputId": "c0403a13-a167-49dc-c5d7-f86eece1dfce"
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOiYm6qcr5cP"
   },
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ecnu-eAQr9mx"
   },
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J53QqDYYr_mL"
   },
   "outputs": [],
   "source": [
    "transformer.fit(train_ds,\n",
    "                epochs=20,\n",
    "                validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yR6j5IwXN6X6",
    "outputId": "00aec948-38c2-4320-dc92-d152270067cc"
   },
   "outputs": [],
   "source": [
    "# history = transformer.fit(\n",
    "#     train_ds,\n",
    "#     epochs=100,\n",
    "#     steps_per_epoch = 100,\n",
    "#     validation_data=val_ds,\n",
    "#     validation_steps = 20,\n",
    "#     callbacks=[\n",
    "#         tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kWi2BzZOeky",
    "outputId": "7540c63e-e25d-4af5-d113-80d49b3c15a3"
   },
   "outputs": [],
   "source": [
    "transformer.evaluate(val_ds, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "q0a05PLMOm9q",
    "outputId": "14c06ceb-bc5c-443c-8f54-8aad228836b2"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQl5nQ_4BdaV",
    "outputId": "c9427907-5af7-450a-b72e-e3fbefc42205"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "CCUHn35vOpFp",
    "outputId": "7d1f8d90-f226-438e-8751-a6e2cfb1178b"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['masked_accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_accuracy'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbDUywGhsZcj"
   },
   "source": [
    "### Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJLaXTaosb4N"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "  def __init__(self, context_text_processor, target_text_processor, transformer):\n",
    "    self.context_text_processor = context_text_processor\n",
    "    self.target_text_processor = target_text_processor\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=max_vocab_size):\n",
    "\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "\n",
    "    sentence = self.context_text_processor(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "\n",
    "    start = tf.constant(target_text_processor.get_vocabulary().index(\"[START]\"), dtype=tf.int64)[tf.newaxis]\n",
    "    end = tf.constant(target_text_processor.get_vocabulary().index(\"[END]\"), dtype=tf.int64)[tf.newaxis]\n",
    "\n",
    "\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "\n",
    "      pre_id = tf.cast(predicted_id[0], dtype=tf.int64)\n",
    "      output_array = output_array.write(i+1, pre_id)\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "\n",
    "\n",
    "    target_vocab = np.array(target_text_processor.get_vocabulary())\n",
    "    tokens_t = target_vocab[output]\n",
    "\n",
    "\n",
    "    text = ' '.join(tokens_t[1:len(tokens_t)-1])\n",
    "    tokens = tokens_t\n",
    "\n",
    "\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zif2R1GoRh5A"
   },
   "outputs": [],
   "source": [
    "@Translator.add_method\n",
    "def plot_attention(self, text, **kwargs):\n",
    "  assert isinstance(text, str)\n",
    "  output = self.translate([text], **kwargs)\n",
    "  output = output[0].numpy().decode()\n",
    "\n",
    "  attention = self.last_attention_weights[0]\n",
    "\n",
    "  context = tf_lower_and_split_punct(text)\n",
    "  context = context.numpy().decode().split()\n",
    "\n",
    "  output = tf_lower_and_split_punct(output)\n",
    "  output = output.numpy().decode().split()[1:]\n",
    "\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  ax.set_xlabel('Input text')\n",
    "  ax.set_ylabel('Output text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh83UKc1vtgv"
   },
   "outputs": [],
   "source": [
    "translator = Translator(context_text_processor, target_text_processor, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyhtJWcnRxBW"
   },
   "outputs": [],
   "source": [
    "translator.plot_attention(' are you still at home? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwbljKeHv4jx"
   },
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {tokens}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "IhXsNIwAv56p",
    "outputId": "17a025d2-aaff-4cd6-d16d-9f561442a8f6"
   },
   "outputs": [],
   "source": [
    "sentence = ' tom likes you '\n",
    "ground_truth = \"  \"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuyj-Euc2suO"
   },
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def __call__(self, sentence):\n",
    "    (result,\n",
    "     tokens,\n",
    "     attention_weights) = self.translator(sentence)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GTGaS-82xCS"
   },
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7lU6_zm2yS4"
   },
   "outputs": [],
   "source": [
    "translator(' tom was very tired . he likes you . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiKyzFsf2zgQ"
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(translator, export_dir='translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0euL_p220po"
   },
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load('translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGsNp1ua23o-"
   },
   "outputs": [],
   "source": [
    "# reloaded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_Kpb-gdsXMk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation methods and benchmarking techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSeRYR15ranR"
   },
   "source": [
    "### Hyperparameter Tuning:\n",
    "Tune the hyperparameters of the model, such as the learning rate, batch size, and number of training epochs, to optimize its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP-g80y2rfew"
   },
   "source": [
    "\n",
    "### Evaluation:\n",
    "Evaluate the performance of the model on the test set using standard metrics such as BLEU, ROUGE, or METEOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "d914b796d66c491eaed9c2d70b2dd916",
      "97f7297ebe70479fafc9c45444e4eac9",
      "621438d887ba44c7ac7eb0aa5190072d",
      "6b8934418df04998a7fdaa7c3b9ed67a",
      "a3f38cfd1a764918905d067598b02541",
      "f9172bcfe15244929b16fef5e8916caa",
      "e849794def5245cb894361fbc5286dc4",
      "00dc5fc07a36451a8c2ff6e40bb9c5e0",
      "c5ef30d8871740fcb726565d622ddaef",
      "b3c879ac813245e18f343e4ce0707c59",
      "4ecdacf9978d454c8ff7347ee3a7987e",
      "172bd93fed2e4dfcbd2204f21ed70e4f",
      "beb9244dcb4043109bf4c15ee991dcd6",
      "d0caff30d97c4a35af207940d4d28664",
      "77b256bc412d4167ad9e15b93e30044e",
      "a5f83a1dd73c4ad89010c2af53f29804",
      "22bea542a7d249b0b8900f9e874d94de",
      "be5e3e6caaba45eca85ddd5e147d89a8",
      "fa53955742c54505a550e0aa6439b237",
      "3081e2d0ecfe4dcdb1d5f306d5200724",
      "5ed8f4eaebab4b089d0b5d657ee6cbc6",
      "59cdd85dfd754c6abc2ee9bb9f35e3b8"
     ]
    },
    "id": "3FPLDd3yGUGN",
    "outputId": "59618d0c-b59f-47d2-fa18-704843c9f1f7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleu = load_metric(\"bleu\")\n",
    "predictions = [[\"the\", \"picture\", \"the\", \"picture\",\n",
    "\t\t\t\t\"by\", \"me\"]]\n",
    "references = [\n",
    "\t[[\"the\", \"picture\", \"is\", \"clicked\", \"by\", \"me\"],\n",
    "\t[\"this\", \"picture\", \"was\", \"clicked\", \"by\", \"me\"]]\n",
    "]\n",
    "print(bleu.compute(predictions=predictions, references=references))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01wKCzKYGXVp"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(predictions, references)\n",
    "rouge_l_score = scores[0][\"rouge-l\"][\"f\"]\n",
    "print(rouge_l_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R81CsmL3nnY4"
   },
   "source": [
    "### Deployment:\n",
    "Deploy the model for use in production by integrating it into a web or mobile application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZoueVi_qAKb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mNKy7p2th5R"
   },
   "source": [
    "## Reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKlkFiiA8buF"
   },
   "source": [
    "https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/bert_glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_hVCMx3_Ty1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "ip7wryjifqbQ",
    "f0d2BmuWf0Vw",
    "t7cF_O1Ea4lV",
    "s32MGXsoEzgr"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00dc5fc07a36451a8c2ff6e40bb9c5e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "172bd93fed2e4dfcbd2204f21ed70e4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_beb9244dcb4043109bf4c15ee991dcd6",
       "IPY_MODEL_d0caff30d97c4a35af207940d4d28664",
       "IPY_MODEL_77b256bc412d4167ad9e15b93e30044e"
      ],
      "layout": "IPY_MODEL_a5f83a1dd73c4ad89010c2af53f29804"
     }
    },
    "22bea542a7d249b0b8900f9e874d94de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3081e2d0ecfe4dcdb1d5f306d5200724": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ecdacf9978d454c8ff7347ee3a7987e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59cdd85dfd754c6abc2ee9bb9f35e3b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ed8f4eaebab4b089d0b5d657ee6cbc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "621438d887ba44c7ac7eb0aa5190072d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00dc5fc07a36451a8c2ff6e40bb9c5e0",
      "max": 2478,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5ef30d8871740fcb726565d622ddaef",
      "value": 2478
     }
    },
    "6b8934418df04998a7fdaa7c3b9ed67a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3c879ac813245e18f343e4ce0707c59",
      "placeholder": "​",
      "style": "IPY_MODEL_4ecdacf9978d454c8ff7347ee3a7987e",
      "value": " 6.06k/? [00:00&lt;00:00, 141kB/s]"
     }
    },
    "77b256bc412d4167ad9e15b93e30044e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ed8f4eaebab4b089d0b5d657ee6cbc6",
      "placeholder": "​",
      "style": "IPY_MODEL_59cdd85dfd754c6abc2ee9bb9f35e3b8",
      "value": " 4.07k/? [00:00&lt;00:00, 181kB/s]"
     }
    },
    "97f7297ebe70479fafc9c45444e4eac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9172bcfe15244929b16fef5e8916caa",
      "placeholder": "​",
      "style": "IPY_MODEL_e849794def5245cb894361fbc5286dc4",
      "value": "Downloading builder script: "
     }
    },
    "a3f38cfd1a764918905d067598b02541": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5f83a1dd73c4ad89010c2af53f29804": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3c879ac813245e18f343e4ce0707c59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be5e3e6caaba45eca85ddd5e147d89a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "beb9244dcb4043109bf4c15ee991dcd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22bea542a7d249b0b8900f9e874d94de",
      "placeholder": "​",
      "style": "IPY_MODEL_be5e3e6caaba45eca85ddd5e147d89a8",
      "value": "Downloading extra modules: "
     }
    },
    "c5ef30d8871740fcb726565d622ddaef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d0caff30d97c4a35af207940d4d28664": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa53955742c54505a550e0aa6439b237",
      "max": 1554,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3081e2d0ecfe4dcdb1d5f306d5200724",
      "value": 1554
     }
    },
    "d914b796d66c491eaed9c2d70b2dd916": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_97f7297ebe70479fafc9c45444e4eac9",
       "IPY_MODEL_621438d887ba44c7ac7eb0aa5190072d",
       "IPY_MODEL_6b8934418df04998a7fdaa7c3b9ed67a"
      ],
      "layout": "IPY_MODEL_a3f38cfd1a764918905d067598b02541"
     }
    },
    "e849794def5245cb894361fbc5286dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9172bcfe15244929b16fef5e8916caa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa53955742c54505a550e0aa6439b237": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
